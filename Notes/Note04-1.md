 # SVM：Support Vector Machine
 
本篇文章是个人学习SVM的学习笔记，正文内容主要记录掌握尚不牢固的知识点和学习时的思考主供个人食用，如需系统了解SVM请移步文末参考资料。  

# 一、摘要
SVM，即支持向量机，是一种有监督学习中的二分类模型，是定义在特征空间上的间隔最大化的线性分类器。它的基本思想为采用最大化间隔策略，
寻找一个最优决策超平面并将所有样本点划分到超平面两侧，实现对数据的分类。最大化间隔策略即求解凸二次规划问题。  

# 二、正文  
> ## 1. 了解SVM  
> 
> ### 1.1 函数间隔(Functional margin)与几何间隔(Geometrical margin)  
> 
>  函数间隔定义为：![Note04-1-13](/Img/Note04-1-13.bmp)   
>  其中，y与f(x)的符号是否一致用于判断分类器的分类是否正确。所求超平面即为使得函数间隔取得最小值的超平面。然而，当w、b成比例放缩时(如2w、2b)，
>  几何间隔也会成相应比例放大缩小，然而所求样本点到超平面的几何距离却并不因此改变，故引入几何间隔这一概念真正定义点到超平面距离这一概念。  
>  
>  几何间隔定义为：![Note04-1-14](/Img/Note04-1-14.bmp)   
>  对于上式，我们既可以结合数学中向量相关的概念从算式层面证明，又可以理解为是点到直线的距离公式拓展到n维空间求点到(n-1)维空间图形的距离。  
>  
>  综上，函数间隔只是人为定义的一个间隔度量，几何间隔才是真正直观意义上的样本点到超平面的距离。
>
> ### 1.2 最大间隔分类器(Maximum Margin Classifier)  
> 
>  根据第1小节中的结论，最大间隔分类器中的“间隔”指的是几何间隔。  
>  
>  ![Note04-1-16](/Img/Note04-1-16.bmp)   
>  
>  而在分类器中，我们为了化简运算，所以把y(wx+b)即函数间隔看作1后进行最大化的求解。这里要注意的是，y(wx+b)的功能主要是确定符号以判定分类器是否分类准确，
>  对于具体的取值其实是不关心的；并且在具体的max求解过程中它对最终结果即目标函数的优化没有影响；对于一个二分类器，只要最终结果取值是二值即可，取y=1/y=-1只是因为此时的y值
>  能区别符号使得结果更加直观清晰。  
>  
>  目标函数转换为：![Note04-1-15](/Img/Note04-1-15.bmp)   
>  值得注意的时，在边界处的样本点即使所说的支持向量，本节中的图是一个硬间隔问题(超平面能使得所有的样本点都被分在间隔两侧)，
>  而对于软间隔问题(部分异常样本点可能在间隔内)间隔内的样本点我们也称之为支持向量。  
>  
>  至此为止，若只关心SVM如何使用则已经足够，不必向下深究更加深层的数学原理。  



> ## 2. 深入SVM  
> 
> ### 2.1 从线性可分到线性不可分  
> 
>  考虑之前提到的目标函数：![Note04-1-15](/Img/Note04-1-15.bmp)   
> 
>  通过数学变换将它转换为：![Note04-1-17](/Img/Note04-1-17.bmp)   
> 
>  这里我们要注意，因为经过了数学变化将原来的分母转化为了分子，因此原先的最大化问题自然也就变成了最小化问题，两问题等价。
>  注意到||w||多了平方，这是因为||w||含有根号，通过平方出去根号可以化简后续运算，且对目标函数的求值没有影响。  
> 
>  现在的目标函数是二次函数，约束条件是线性的，因此这是一个凸二次规划问题(满足某些条件，目标最优，损失最小)。 
> 
>  这里我们需要应用*拉格朗日对偶法*对问题进行化简求值。由于这个问题的特殊结构，我们运用拉格朗日乘子法得到原问题的对偶问题，通过求解对偶问题得到原问题的最优解，
>  这就是线性可分条件下支持向量机的对偶算法，这样做的优点在于：对偶问题往往更容易求解；可以自然地引入核函数，进而推广到非线性分类问题。  
>  构造拉格朗日函数：![Note04-1-18](/Img/Note04-1-18.bmp)   
>  令：![Note04-1-19](/Img/Note04-1-19.bmp)   
>  得到目标函数：![Note04-1-20](/Img/Note04-1-20.bmp)   
>  得到对偶问题：![Note04-1-21](/Img/Note04-1-21.bmp)   
>  之所以转换为对偶问题，是因为原问题需要求w和b两个优化变量，而对偶问题只需要求出拉格朗日乘子alpha即可。  
> 
>  值得注意的是，  
>  * 在不等式优化问题中，仅在强对偶条件下才有min maxf = max minf，而KTT条件为不等式优化为问题强对偶性的充要条件。
>  (这里注意到“满足某些条件”是否指KTT条件对于不同的笔记文章有不同的解释，细节如何笔者还没搞懂，未来再进行更正补充，不影响SVM的实际应用这里暂不展开)  
>  * 解决不等式优化问题，通常先引入松弛变量将其转化为等式约束优化，后引入拉格朗日乘子转为无约束优化(对所有优化变量求偏导并令其值为0).  
>  



> ## 支持向量和最大化间隔  
> 
> 超平面：n维线性空间中维度为n-1的子空间，把线性空间分割成不相交的两部分。  
> ![Note04-1-1](/Img/Note04-1-1.bmp)  
> 
>  SVM算法目标：寻找一个超平面将数据集D划分为两个不相交的子集  
>  ![Note04-1-2](/Img/Note04-1-2.bmp)  
>  ![Note04-1-3](/Img/Note04-1-3.bmp)  
>
> 间隔最大化：选择离两边数据集都尽量远的超平面来划分数据空间  
> 支持向量：利用间隔最大化方法选取超平面后，两个数据集中离超平面最近的点  
>  ![Note04-1-4](/Img/Note04-1-4.bmp)  
>
> 问题转化为约束最优化问题：  
>  ![Note04-1-5](/Img/Note04-1-5.bmp)  
>  上式表明，求有多个超平面的多个取值，找到最优的取值，即找到几何间隔最大化的超平面。  
>  当参数w和b按照一定比例缩放时，函数距离也会跟着比例缩放，但所确定的超平面不会改变。
>  故为化简计算进行数学变化，缩放函数距离到1，最优化问题化简为：  
>   ![Note04-1-6](/Img/Note04-1-6.bmp)  
>   上述最大化问题与下述最小值优化问题等价（凸二次优化问题）（数学上的变化？）： 
>   ![Note04-1-7](/Img/Note04-1-7.bmp)   
>   
> ## 优化与求解  
> 凸二次优化问题：目标函数是二次函数且约束函数是线性的。  
> 这类问题通常采用引入拉格朗日乘子法进行转化，将有约束条件的问题转化为无约束条件的问题进行求解：  
>  ![Note04-1-8](/Img/Note04-1-8.bmp)   
>
> 此时对参数w和b已经没有约束条件。根据拉格朗日对偶性，将上式中求最大值的最小值问题转化为求最小值的最大值问题：  
>  ![Note04-1-9](/Img/Note04-1-9.bmp)   
>  ![Note04-1-10](/Img/Note04-1-10.bmp)   
>  ![Note04-1-11](/Img/Note04-1-11.bmp)   
>  ![Note04-1-12](/Img/Note04-1-12.bmp)   
>
> 训练完成后，大部分训练样本均不保留，最终模型仅与支持向量有关，占用内存少，预测快；  
> 适合数据维度高，数据量小的情况，因为调参耗时，时间复杂度大。  

# 参考资料  
1. 适用于已经了解SVM用作复习使用：[机器学习回顾篇：支持向量机SVM](https://www.cnblogs.com/chenhuabin/p/11986889.html)  
2. 对相关数学知识已有一定基础了解但对SVM具体结构不大了解：[【机器学习】支持向量机SVM](https://zhuanlan.zhihu.com/p/77750026)  
3. 详细讲解SVM中应用的各种数学知识，一文由浅入深掌握SVM：[支持向量机通俗导论](https://blog.csdn.net/v_july_v/article/details/7624837)   

